{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\t'''\n",
    "\tThe positional encoding class is used in the encoder and decoder layers.\n",
    "\tIt's role is to inject sequence order information into the data since self-attention\n",
    "\tmechanisms are permuatation equivariant. Naturally, this is not required in the static\n",
    "\ttransformer since there is no concept of 'order' in a portfolio.'''\n",
    "\n",
    "\tdef __init__(self, window, d_model):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.register_buffer('d_model', torch.tensor(d_model, dtype = torch.float64))\n",
    "\n",
    "\t\tpe = torch.zeros(window, d_model)\n",
    "\t\tfor pos in range(window):\n",
    "\t\t\tfor i in range(0, d_model, 2):\n",
    "\t\t\t  pe[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "\t\t\t\t\n",
    "\t\t\tfor i in range(1, d_model, 2):\n",
    "\t\t\t  pe[pos, i] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))             \n",
    "\t\t\t\t\n",
    "\t\tpe = pe.unsqueeze(0)\n",
    "\t\tself.register_buffer('pe', pe)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x * torch.sqrt(self.d_model) + self.pe[:,:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(seq_len):\n",
    "\t'''\n",
    "\tCreate a mask to be used in the decoder.\n",
    "\tReturns a mask of shape (1, seq_len, seq_len)\n",
    "\t'''\n",
    "\tno_peak_mask = np.triu(np.ones((seq_len, seq_len)), k = 1).astype('uint8')\n",
    "\treturn torch.from_numpy(no_peak_mask)\n",
    "\n",
    "def get_clones(module, N):\n",
    "\t'''\n",
    "\tThis helper function is used to create copies of encoder and decoder layers.\n",
    "\tThese copies of encoder/decoder layers are used to construct the\n",
    "\tcomplete stacked encoder/decoder modules.\n",
    "\t'''\n",
    "\treturn nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(k, q, v, mask = None):\n",
    "\t'''\n",
    "\tk : (batch, seq_len_k, heads, d_model)\n",
    "\tq : (batch, seq_len_q, heads, d_model)\n",
    "\tv : (batch, seq_len_v, heads, d_model)\n",
    "\trequire seq_len_k == seq_len_v\n",
    "\t'''\n",
    "\n",
    "\tb, _, h, d = k.shape\n",
    "\n",
    "\tk = k.transpose(1, 2).contiguous().view(b * h, -1, d)\n",
    "\tq = q.transpose(1, 2).contiguous().view(b * h, -1, d)\n",
    "\tv = v.transpose(1, 2).contiguous().view(b * h, -1, d)\n",
    "\t\n",
    "\tscores = torch.matmul(q, k.transpose(1, 2))\n",
    "\tif mask is not None:\n",
    "\t\tscores = scores.masked_fill(mask == 0, -1e9)\n",
    "\tscores = F.softmax(scores,dim=2)\n",
    "\n",
    "\t# Scaled dot-product.\n",
    "\tscores = torch.matmul(scores, v).view(b, h, -1, d)\n",
    "\treturn scores.transpose(1, 2).contiguous().view(b, -1, h * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\t'''This is a Mult-Head wide self-attention class.'''\n",
    "\tdef __init__(self, heads, d_model, dropout = 0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.h = heads\n",
    "\t\tself.d_model = d_model\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\t\tself.q_linear = nn.Linear(d_model, heads * d_model,bias=False)\n",
    "\t\tself.v_linear = nn.Linear(d_model, heads * d_model,bias=False)\n",
    "\t\tself.k_linear = nn.Linear(d_model, heads * d_model,bias=False)\n",
    "\t\t  \n",
    "\t\tself.unifyheads = nn.Linear(heads * d_model, d_model)\n",
    "\n",
    "\tdef forward(self, q, k, v, mask = None):\n",
    "\n",
    "\t\tb = q.shape[0]\n",
    "\n",
    "\t\tk = self.k_linear(k).view(b, -1, self.h, self.d_model)\n",
    "\t\tq = self.q_linear(q).view(b, -1, self.h, self.d_model)\n",
    "\t\tv = self.v_linear(v).view(b, -1, self.h, self.d_model)\n",
    "\n",
    "\t\toutput = scaled_dot_product_attention(k, q, v, mask = mask)\n",
    "\t\toutput = self.unifyheads(output)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\t'''This is a pointwise feedforward network.'''\n",
    "\tdef __init__(self, d_model, dff, dropout = 0.1):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.ff = nn.Sequential(\n",
    "\t\t\tnn.Linear(d_model, dff),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(dropout),\n",
    "\t\t\tnn.Linear(dff, d_model))\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.ff(x)\n",
    "\t\treturn x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\t'''Encoder layer class.'''\n",
    "\tdef __init__(self, heads, d_model, dff, dropout = 0.1):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.norm_1 = nn.LayerNorm(d_model)\n",
    "\t\tself.norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "\t\tself.attn = MultiHeadAttention(heads, d_model, dropout = dropout)\n",
    "\t\tself.ff = FeedForward(d_model, dff)\n",
    "\n",
    "\t\tself.dropout_1 = nn.Dropout(dropout)\n",
    "\t\tself.dropout_2 = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tattn_out = self.dropout_1(self.attn(x, x, x))\n",
    "\t\tx = self.norm_1(x + attn_out)\n",
    "\n",
    "\t\tffn_out = self.ff(x)\n",
    "\t\tx = self.norm_2(x + ffn_out)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\t'''Stacked encoder layers.'''\n",
    "\tdef __init__(self, N, pe_window, heads, inp_dim, d_model, dff, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.N = N\n",
    "\t\tself.embedding = nn.Linear(inp_dim, d_model)  # change the meaning of embedding in finance data\n",
    "\t\tself.pe = PositionalEncoding(pe_window, d_model)\n",
    "\t\tself.dynamiclayers = get_clones(EncoderLayer(heads, d_model, dff, dropout = dropout), N)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# x (batch, seq_len, inp_dim)\n",
    "\n",
    "\t\tx = self.embedding(x) # (batch, seq_len, d_model)\n",
    "\t\t\n",
    "\t\tx = self.pe(x) # (batch, seq_len, d_model)\n",
    "\n",
    "\t\tfor i in range(self.N):\n",
    "\t\t  x = self.dynamiclayers[i](x) # (batch, seq_len, d_model)\n",
    "\n",
    "\t\treturn x # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\t'''Decoder Layer class'''\n",
    "\tdef __init__(self, heads, d_model, dff, dropout = 0.1):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.norm_1 = nn.LayerNorm(d_model)\n",
    "\t\tself.norm_2 = nn.LayerNorm(d_model)\n",
    "\t\tself.norm_3 = nn.LayerNorm(d_model)\n",
    "\n",
    "\t\tself.attn_1 = MultiHeadAttention(heads, d_model, dropout = dropout)\n",
    "\t\tself.attn_2 = MultiHeadAttention(heads, d_model, dropout = dropout)\n",
    "\t\tself.ff = FeedForward(d_model, dff)\n",
    "\n",
    "\t\tself.dropout_1 = nn.Dropout(dropout)\n",
    "\t\tself.dropout_2 = nn.Dropout(dropout)\n",
    "\t\tself.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "\tdef forward(self, x, enc_out, mask = None):\n",
    "\t\t# x (batch, seq_len, d_model)\n",
    "\t\t# enc_out (batch, enc_seq_len, d_model)\n",
    "\n",
    "\t\tattn_1_out = self.dropout_1(self.attn_1(x, x, x, mask = mask))\n",
    "\t\tx = self.norm_1(x + attn_1_out) # (batch, seq_len, d_model)\n",
    "\n",
    "\t\tattn_2_out = self.dropout_2(self.attn_2(x, enc_out, enc_out))\n",
    "\t\tx = self.norm_2(x + attn_2_out) # (batch, seq_len, d_model)\n",
    "\n",
    "\t\tffn_out = self.dropout_3(self.ff(x))\n",
    "\t\tx = self.norm_3(x + ffn_out) # (batch, seq_len, d_model)\n",
    "\n",
    "\t\treturn x # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\t'''Stacked decoder layers.'''\n",
    "\tdef __init__(self, N, pe_window, heads, inp_dim, d_model, dff, dropout = 0.1):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.N = N\n",
    "\t\tself.embedding = nn.Linear(inp_dim, d_model)\n",
    "\t\tself.pe = PositionalEncoding(pe_window, d_model)\n",
    "\t\tself.decoderlayers = get_clones(DecoderLayer(heads, d_model, dff, dropout = dropout), N)\n",
    "\n",
    "\tdef forward(self, x, enc_out, mask = None):\n",
    "\t\t# x (batch, seq_len, inp_dim)\n",
    "\t\t# enc_out (batch, enc_seq_len, d_model)\n",
    "\n",
    "\t\tx = self.embedding(x) # (batch, seq_len, d_model)\n",
    "\n",
    "\t\tx = self.pe(x) # (batch, seq_len, d_model)\n",
    "\t\t\n",
    "\t\tfor i in range(self.N):\n",
    "\t\t\tx = self.decoderlayers[i](x, enc_out, mask = mask) # (batch, seq_len, d_model)\n",
    "\t\t\n",
    "\t\treturn x # (batch, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ml4fTransformer(nn.Module):\n",
    "\t'''\n",
    "\tMain transformer class.\n",
    "\texperiment : selects sigmoid final activation if 'movement' else linear\n",
    "\tinp_dim_e : number of dimensions of encoder input\n",
    "\tinp_dim_d : number of dimensions of decoder input\n",
    "\td_model : model embedding dimension\n",
    "\tdff : hidden dimension of feed forward network\n",
    "\tN_e : number of encoder layers\n",
    "\tN_d : number of decoder layers\n",
    "\theads : number of heads\n",
    "\t'''\n",
    "\tdef __init__(self, inp_dim_e, inp_dim_d, d_model = 20,\n",
    "\t\tdff = 80, N_e = 1, N_d = 1, heads = 4, dropout = 0.1, pe_window = 100):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tassert d_model % heads == 0\n",
    "\t\t\n",
    "\t\tself.encoder = Encoder(N_e, pe_window, heads, inp_dim_e, d_model, dff, dropout = dropout)\n",
    "\t\t\n",
    "\t\tself.decoder = Decoder(N_d, pe_window, heads, inp_dim_d, d_model, dff, dropout = dropout)\n",
    "\t\t\n",
    "\t\tself.map = nn.Linear(d_model, inp_dim_d)\n",
    "\t\n",
    "\tdef forward(self, x, y, mask = None):\n",
    "\t\t'''\n",
    "\t\tx (batch, in_seq_len, inp_dim_e)\n",
    "\t\ty (batch, tar_seq_len, inp_dim_d)\n",
    "\t\t'''\n",
    "\n",
    "\t\tenc_out = self.encoder(x) # (batch, in_seq_len, d_model)\n",
    "\n",
    "\t\tdec_out = self.decoder(y, enc_out, mask = mask) # (batch, tar_seq_len, d_model)\n",
    "\n",
    "\t\tfinal = self.map(dec_out) # (batch, tar_seq_len, out_dim_d)\n",
    "\n",
    "\t\treturn final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrSchedule:\n",
    "\t'''\n",
    "\tImplements the learning rate schedule from \"Attention is all you need\".\n",
    "\tLearning rate given by (1/sqrt(d_model)) * min(steps^-0.5, steps * warmup_steps ^-1.5).\n",
    "\tInit with a pytorch.optim optimizer then call as usual in the training loop.\n",
    "\t'''\n",
    "\tdef __init__(self, optimizer, warmup_steps, d_model, scale = 1.0):\n",
    "\t\tself.optimizer_ = optimizer \n",
    "\t\tself.warmup_steps = warmup_steps\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.scale = scale\n",
    "\t\tself.step_ = 1\n",
    "\n",
    "\tdef zero_grad(self):\n",
    "\t\tself.optimizer_.zero_grad()\n",
    "\n",
    "\tdef get_lr(self, step):\n",
    "\t\tmin_ = np.minimum(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "\t\treturn (self.d_model ** -0.5) * min_\n",
    "\n",
    "\tdef update_lr(self):\n",
    "\t\tlr = self.scale * self.get_lr(self.step_)\n",
    "\t\tself.step_ += 1\n",
    "\n",
    "\t\tfor param_group in self.optimizer_.param_groups:\n",
    "\t\t\tparam_group['lr'] = lr\n",
    "\n",
    "\tdef step(self):\n",
    "\t\tself.update_lr()\n",
    "\t\tself.optimizer_.step()\n",
    "\n",
    "\tdef reset_lr(self, lr = 1e-3):\n",
    "\t\tfor param_group in self.optimizer_.param_groups:\n",
    "\t\t\tparam_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3571: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "result = pd.read_csv('data/1_before2016.csv')\n",
    "data = result.drop('meanest', axis=1)\n",
    "x = data.drop('actual', axis=1)\n",
    "mean_normal = np.mean(x, axis=0)\n",
    "std_normal = np.std(x, axis=0)\n",
    "data.loc[:, 'actq':'fincfy'] = (x - mean_normal)/std_normal\n",
    "\n",
    "xseq = []\n",
    "yseq = []\n",
    "for name, content in data.groupby('tic'):\n",
    "    for i in range(len(content.index)-10):\n",
    "        train_seq = []\n",
    "        train_label = []\n",
    "        for j in range(i, i + 10):\n",
    "            train_seq.append(content.iloc[j]['actual'])\n",
    "        for j in range(i+1, i + 11):\n",
    "            train_label.append(content.iloc[j]['actual'])\n",
    "        xseq.append(train_seq)\n",
    "        yseq.append(train_label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = pd.read_csv('data/1_after2016.csv')\n",
    "data1 = result1.drop('meanest', axis=1)\n",
    "x1 = data1.drop('actual', axis=1)\n",
    "data1.loc[:, 'actq':'fincfy'] = (x1 - mean_normal)/std_normal\n",
    "\n",
    "xseqt = []\n",
    "yseqt = []\n",
    "for name, content in data1.groupby('tic'):\n",
    "    for i in range(len(content.index)-10):\n",
    "        train_seq = []\n",
    "        train_label = []\n",
    "        for j in range(i, i + 10):\n",
    "            train_seq.append(content.iloc[j]['actual'])\n",
    "        for j in range(i+1, i + 11):\n",
    "            train_label.append(content.iloc[j]['actual'])\n",
    "        xseqt.append(train_seq)\n",
    "        yseqt.append(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model = Ml4fTransformer(1, 1)\n",
    "\n",
    "train = [(xseq[i], yseq[i]) for i in range(len(xseq))]\n",
    "random.shuffle(train)\n",
    "\n",
    "N = len(xseq)\n",
    "train_N = (int(N/64)-20) * 64\n",
    "valid_N = N - train_N\n",
    "\n",
    "train_x = torch.FloatTensor([train[i][0] for i in range(train_N)])\n",
    "train_x = train_x.reshape(train_x.shape[0], -1, 1)\n",
    "train_y = torch.FloatTensor([train[i][1] for i in range(train_N)])\n",
    "train_y = train_y.reshape(train_y.shape[0], -1, 1)\n",
    "\n",
    "valid_x = torch.FloatTensor([train[i][0] for i in range(train_N, N)])\n",
    "valid_x = valid_x.reshape(valid_x.shape[0], -1, 1)\n",
    "valid_y = torch.FloatTensor([train[i][1] for i in range(train_N, N)])\n",
    "valid_y = valid_y.reshape(valid_y.shape[0], -1, 1)\n",
    "\n",
    "test_x = torch.FloatTensor(xseqt)\n",
    "test_y = torch.FloatTensor(yseqt)\n",
    "test_y = test_y.reshape(test_y.shape[0], -1, 1)\n",
    "\n",
    "mask = create_mask(train_y.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = LrSchedule(optimizer, 1e-1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16896, 10, 1]),\n",
       " torch.Size([16896, 10, 1]),\n",
       " torch.Size([1333, 10, 1]),\n",
       " torch.Size([1333, 10, 1]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 162.93341064453125\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'minloss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m v_out \u001b[39m=\u001b[39m model(v_src, v_tgt, mask)\n\u001b[1;32m     33\u001b[0m v_loss \u001b[39m=\u001b[39m criteria(v_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :], v_tgt_y[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[0;32m---> 34\u001b[0m \u001b[39mif\u001b[39;00m(v_loss \u001b[39m<\u001b[39m minloss):\n\u001b[1;32m     35\u001b[0m   minloss \u001b[39m=\u001b[39m v_loss\n\u001b[1;32m     36\u001b[0m   \u001b[39m#torch.save(model, 'drive/MyDrive/transformer.pth')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minloss' is not defined"
     ]
    }
   ],
   "source": [
    "minloss = 100\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for step in range(int(train_x.shape[0]/64)):\n",
    "        # 生成数据\n",
    "        src = train_x[step*64 : (step*64+64)]\n",
    "        tgt = train_y[step*64 : (step*64+64), :-1, :]\n",
    "        tgt_y = train_y[step*64 : (step*64+64), 1:, :]\n",
    "        # 清空梯度\n",
    "        scheduler.zero_grad()\n",
    "        # 进行transformer的计算\n",
    "        out = model(src, tgt, mask)\n",
    "    \n",
    "        #print(out.contiguous().view(-1, out.size()[-1]))\n",
    "        #print(tgt_y.contiguous().view(-1, tgt_y.size()[-1]))\n",
    "        loss = criteria(out[:, -1, :], tgt_y[:, -1, :])\n",
    "        #print(loss)\n",
    "\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {}, train_loss: {}\".format(epoch, total_loss))\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            v_src = valid_x\n",
    "            v_tgt = valid_y[:, :-1, :]\n",
    "            v_tgt_y = valid_y[:, 1:, :]\n",
    "            v_out = model(v_src, v_tgt, mask)\n",
    "            v_loss = criteria(v_out[:, -1, :], v_tgt_y[:, -1, :])\n",
    "            if(v_loss < minloss):\n",
    "              minloss = v_loss\n",
    "              #torch.save(model, 'drive/MyDrive/transformer.pth')\n",
    "            print(\"Epoch {}, valid_loss: {}\".format(epoch, v_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinten(y_true, y_pred):\n",
    "    sum1 = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] < 1.1*y_true[i] and y_pred[i] > 0.9*y_true[i]):\n",
    "            sum1+=1\n",
    "    return sum1/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def test(model, test_x, test_y):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      v_src = test_x\n",
    "      v_tgt = test_y[:, :-1, :]\n",
    "      v_tgt_y = test_y[:, 1:, :]\n",
    "      v_out = model(v_src, v_tgt, mask)\n",
    "      v_loss = criteria(v_out[:, -1, :], v_tgt_y[:, -1, :]).item()\n",
    "      r2 = r2_score(v_tgt_y[:, -1, :], v_out[:, -1, :])\n",
    "      within10 = withinten(v_tgt_y[:, -1, :], v_out[:, -1, :])\n",
    "  return v_loss, r2, within10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9408961534500122, 0.25726783087586114, 0.1706792777300086)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, r2, within10 = test(model, test_x, test_y)\n",
    "loss, r2, within10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
